---
layout: post
title: Master Thesis
use_math: true
---

This is a blog post describing my master thesis project on View Synthesis in the context of Multiview Video $$($$carried out in 2015$$)$$. The aim of this post is to present the basic idea of the project and not to be a math lecture so expect not to be swamped with equations. 
<figure><img src="/images/3dtv.jpg" alt="3D-TV" style="width: 200px;"/></figure>
Multiview Video Coding is a technique that allows viewers to interactively navigate through different viewpoints of a scene. Imagine watching a basketball game and being able to freely change your viewpoint from one end of the court to the other in a smooth way. This means that you are not restricted to choose solely the viewpoints captured by actual cameras but also all those in between. But all the intermediate viewpoints need to be rendered somehow. This is where the process of view synthesis comes to play.

We refer to the captured viewpoints as reference and to the reconstructed ones as virtual.

A way to synthesize all views in a scene is to explicitly construct a 3d model of the scene and then render the viewpoint chosen by the user. But model-based techniques often oversimplify natural scenes and are computationally expensive. There is an alternative image-based technique that exploits depth in order to translate pixels from the reference images into the virtual viewpoint. $$($$note: a depth map is an image whose pixel values represent the distance of each position wrt the camera$$)$$

<figure><img src="/images/flower-depth.png" alt="depth image" style="width: 200px;"/></figure>

Intuitively, we can understand this by considering the following scene of the ballet dancer. The first photo was captured at a viewpoint a little more to the right from the second one. However, moving all pixels of the first image equally to the right will not give us the second image. We see that pixels of the man $$($$closer to the camera$$)$$ are much more displaced than pixels of the girl $$($$further back$$)$$. In fact, the larger the distance from the camera, the larger the dispacement in the image space. So a depth image is suitable to project the pixels from the reference to the virtual viewpoint. $$($$Hence, the technique is named depth image-based rendering or DIBR$$)$$

<table style="width:100%">
  <tr>
    <td><img src="/images/color-cam0-f000.jpg" alt="depth image" style="width: 200px;"/></td>
    <td><img src="/images/color-cam2-f000.jpg" alt="depth image" style="width: 200px;"/></td> 
    <td><img src="/images/color-cam0-f000_2.jpg" alt="depth image" style="width: 200px;"/></td>
  </tr>
  <tr>
    <td>Image 1 - right</td>
    <td>Image 2 - left</td>
    <td>Their superposition</td>
  </tr>
  <!--<tr>
    <td>Eve</td>
    <td>Jackson</td>
    <td>94</td>
  </tr>
  <tr>
    <td>John</td>
    <td>Doe</td>
    <td>80</td>
  </tr>-->
</table>

The problem that arises is that $$($$exactly due to the unequal displacements of FG-BG$$)$$ some parts of the scene that were occluded from the reference viewpoint, become exposed in the virtual one, creating the so called disocclusion holes. In the case of a single reference viewpoint disocclusion is handled via inpainting.

Inpainting essentially restores or fills-in missing info in an image and, apart from disocclusion handling in DIBR, it has many applications like image restoration and text or object removal.

Images

<table style="width:100%; background-color: #CCCCCC">
  <tr>
    <th><b>Box: Inpainting</b></th>
  </tr>
  <tr>
    <td>Let's call the image I with known part Φ, missing part Ω and δΩ being their border. We want to synthesize texture in Ω by gradually completing it from the outside inwards. Research has showed that the order with which the patches get filled in is critical, because each match will influence all future ones. Priority of each patch Ψp is calculated as the product of two terms:</td>
    <td>Confidence term: quantifies the amount of reliable information the patch contains (equation)</td>
    <td>Data term: measures the existence of sharp edges perpenticular to the contour δΩ. equation $$ D(\mathbf(p))=\fraq{\abs{\nabla I^{\bot}_{\mathbf{p}}}}{\alpha} $$</td>
  </tr>
</table>

Data term: measures the existence of sharp edges perpenticular to the contour δΩ. equation $$ D(\mathbf(p))=\fraq{\abs{\nabla I^{\bot}_{\mathbf{p}}}}{\alpha} $$
Image of notations pg.26/82
End of Box


Inpainting will be applied in the blank areas of the reconstructed image. But what if there are parts that are not blank and still need inpainting?

image pg.40/82


What this work studies is if and how we can use skeletons $$($$pose estimation information$$)$$ along with depth maps to enhance the inpainting process in Multiview over the areas containing human figures. Advantages: Oftentimes the most intersting part in a scene are people, most depth cameras also provide skeletal tracking, it is done real-time so no extra delay, skeletons can often be recovered even for parts of the body not visible in the camera's view.

So how do we exploit the information of the pose to aid our inpainting?
Firstly, we can detect parts of the skeleton that are exposed $$($$their current depth corresponds to the BG and not in the depth range of the rest of the figure$$)$$. So we form masks around the exposed parts that must be filled-in. Secondly, instead of using existing inpainting methods for the filling-in of the masks, we guide the process to prioritize patches containing edges parallel to the skeleton. So for example a missing leg will be completed first by the patches of the contour and then the rest of the in and out patches. To do this a new term is introduced in the priority computation:
Orientation term: favors pathces with edges parallel to the $$($$exposed$$)$$ skeleton. (equation)


So this was the gist. For any details this is my [full report](http://vivliothmmy2.ee.auth.gr/wp-content/uploads/participants-database/kasimidou_eleftheria_dibr.pdf)



Equation here $$x_1$$

$$
   |\psi_1\rangle = a|0\rangle + b|1\rangle
$$

lala
